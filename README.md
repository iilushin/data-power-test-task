# Data Power.  Тестовое задание: загрузка заказов в SQLite

## Описание

Приложение реализует простой ETL-процесс:

1. Чтение входного JSON-файла
2. Приведение структуры и формата данных, очистка данных, дедупликация данных
3. Загрузка подготовленных данных в таблицу SQLite
4. Выгрузка загруженных данных в csv-файл (опционально)

Поддерживает:
* конфигурацию путей, наименование файлов, параметров запуска, настройку таблицы с помощью файла `configs/config.py`;
* автоматическую валидацию путей и создание отсутствующих директорий при необходимости;
* гибкое логирование через конфиг `configs/logging.yaml`;
* Jinja шаблоны SQL;
* дедупликацию по нескольким колонкам и/или первичному ключу.

Ограничения:
* работает только с одной структурой json, как в файле `data/input_data/ozon_orders.json`;
* работает только с одним входным файлом единомоментно;
* нельзя создать составной Primary Key для таблицы в SQLite;
* при дедупликации в выборку попадает первая попавшаяся запись, нельзя указывать условие фильтрации;
* нельзя менять схему уже существующей таблицы.

---

# Запуск локально

## Требования

* Python 3.10+
* pip

## Установка зависимостей

Создать виртуальное окружение:

```
python -m venv .venv
```

Активировать виртуальное окружение:

**Linux/macOS**

```
source .venv/bin/activate
```

**Windows**

```
.venv\Scripts\activate
```

Установить зависимости:

```
pip install -r requirements.txt
```

## Запуск (производится из корня проекта)

```
python main.py
```
---
# Запуск через Docker

## 1. Собрать контейнер

```
docker compose build
```

## 2. Запустить

```
docker compose up
```
или для фонового запуска
```
docker compose up -d
```

После завершения работы программы можно найти логи в директории logs (по умолчанию)

---
# Более детальное описание проекта

Конфигурации для проекта (`config.py`) и логгера (`logging.yaml`) находятся в директории `configs/`.  
Входные и выходные данные по умолчанию находятся в директории `data/`.
Логи по умолчанию создаются в директории `logs/` с дефолтным названием `etl_logs.log`.  
Шаблоны SQL-запросов находятся в директории `sql_queries/`

Проект разбит на несколько модулей, каждый из которых выполняет свою функцию:
* `db/db.py` - отвечает за взаимодействие с бд и выполнение запросов;
* `etl/etl.py` - отвечает за чтение входных данных и их обработку (форматирование, дедупликация, подготовка к вставке в бд);
* `logger/logger.py` - отвечает за настройку логгера для всего проекта;
* `utils/render_sql_templates.py` и `utils/validator.py` - вспомогательные модули для рендера SQL-запросов, валидации путей и конфига в целом;
* `main.py` - точка входа в программу, связывает все модули вместе.  


---
# Как работает дедупликация

### Механизм 1: Первичный ключ

В `config.py` можно указать первичный ключ для колонки:

```python
"order_id": {"data_type": "TEXT", "is_PK": True}
```

При вставке данных в таблицу будет использоваться SQL-шаблон с выражением `INSERT OR IGNORE`, что гарантирует игнорирование дубликатов по ключу.  
**Важно!** В текущей реализации нельзя сделать составной ключ, то есть PK в таблице должен быть один, иначе будет получена ошибка.

### Механизм 2: Перечисление колонок DEDUPLICATE_DATA_BY_COLUMNS
В `config.py` можно указать колонки, по которым будет происходить дедупликация:
```python
DEDUPLICATE_DATA_BY_COLUMNS = ("order_id", "status",)
```

Найденные во входном файле дубликаты не будут записаны таблицу, с учетом имеющихся записей в таблице.  
Как происходит дедупликация: получение колонок `DEDUPLICATE_DATA_BY_COLUMNS` из таблицы -> фильтрация входных данных по комбинации указанных колонок -> запись уникальных строк в таблицу.  
**Важно!** Если в таблице присутствует PK, дедупликация будет производиться с помощью обоих механизмов.  
Если пользователь укажет колонку для дедупликации, которой нет в таблице, программа не будет работать.

---


